# Decision Tree

### pseudo code: 

```python

def createBranch():
	检测数据集中的每个元素是否属于同一个分类
    if so:
        return classLabels
    else:
        寻找划分数据集的最好特征
        划分数据集
        创建分支节点
        	for 每个划分的子集
            	调用createBranch，增加返回结果到分支节点中
        return 分支节点
```



### 信息增益、熵

$假设x_i为待分类的事物，且可能划分到多个分类之中，则x_i的信息定义式为：$
$$
l(x_i) = -\log_2p(x_i),p(x_i):选择该分类的概率
$$
熵：信息的期望值，计算式如下
$$
H = -\sum_{i=1}^np(x_i)\log_2p(x_i),n:分类的数目
$$


tips：基尼不纯度：



### 步骤——使用算法：ID3
缺点：无法直接处理数值型数据
后续讨论剪枝
1. 获取原始数据集

2. 基于最好的属性值划分数据集

3. 判断：

   是否遍历完所有划分数据集的属性

   每个分支下的所有实例都为相同分类——得到叶节点（当前为最后的叶节点则终止）